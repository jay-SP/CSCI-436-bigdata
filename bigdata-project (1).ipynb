{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgfaLD64vGfy"
   },
   "source": [
    "## Big Data Project on Predicting Taxi Fare Price in city of Chicago using Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCXBp-sJvTdT"
   },
   "source": [
    "Getting the Data\n",
    "Get the data from\n",
    "https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=chicago_taxi_trips&page=dataset&project=big-data-project-396823&supportedpurview=project&ws=!1m9!1m4!4m3!1sbigquery-public-data!2schicago_taxi_trips!3staxi_trips!1m3!3m2!1sbigquery-public-data!2schicago_taxi_trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV1SP9a6vff3"
   },
   "source": [
    "The data has the following\n",
    "1.   Total logical bytes: 75.75 GB\n",
    "2.   Number of rows: 208,943,621\n",
    "\n",
    "For the sake of this project lets extract only 5000 rows from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLqkz-7TwNtl"
   },
   "source": [
    "# Use Datalake on AWS S3 to store data for the following reasons\n",
    "\n",
    "## Schema Evolution:\n",
    "Data lakes enable schema-on-read, meaning that you can apply structure to the data during analysis rather than enforcing a fixed schema on ingest. This flexibility is beneficial when dealing with evolving data sources and schema changes over time.\n",
    "\n",
    "## Cost-Efficiency for Storage:\n",
    "Data lakes, like Amazon S3, offer cost-effective storage options for large volumes of data. Since the Chicago taxi fare data might grow over time, you can leverage a pay-as-you-go pricing model, storing the data without incurring significant costs.\n",
    "\n",
    "## Handling High Volume and Velocity:\n",
    "If you're dealing with large volumes of data or high data velocity (frequent updates), data lakes can handle the scale more effectively. They're designed to handle big data scenarios and can accommodate rapid growth.\n",
    "\n",
    "## Scalability and Future-Proofing:\n",
    "Data lakes offer high scalability and can adapt to future data needs. As new data sources emerge and analytical requirements evolve, a data lake can provide a more scalable and adaptable solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31Tv1c5QxV-8"
   },
   "source": [
    "##install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30235,
     "status": "ok",
     "timestamp": 1692942071658,
     "user": {
      "displayName": "anudeep anisetty",
      "userId": "13005275210132423070"
     },
     "user_tz": 420
    },
    "id": "PNHv3wAoSTyd",
    "outputId": "f080254f-5b02-4100-f502-c982a260f6b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285388 sha256=a89313ac234b1e342e1855afdcda1bf6a99b34c7851b2bedf44ac2239bcf4c05\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.4.1\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EccSagyxapj"
   },
   "source": [
    "## import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1692950864476,
     "user": {
      "displayName": "anudeep anisetty",
      "userId": "13005275210132423070"
     },
     "user_tz": 420
    },
    "id": "pru-Jz50tN5-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5nhDXryxiuv"
   },
   "source": [
    "#Download the data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1692950867992,
     "user": {
      "displayName": "anudeep anisetty",
      "userId": "13005275210132423070"
     },
     "user_tz": 420
    },
    "id": "EmWK1sXUt9E_",
    "outputId": "3d54462b-3be8-4ed5-e215-d80cd2087a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://fractalcards-dev.s3.amazonaws.com/Chicago_taxi_fare_5000.csv\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"Chicago_taxi_fare_5000.csv\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"CSV file downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70nxQ04_xrFr"
   },
   "source": [
    "start spark with the app name PricePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1692950870697,
     "user": {
      "displayName": "anudeep anisetty",
      "userId": "13005275210132423070"
     },
     "user_tz": 420
    },
    "id": "7pl57a22k-Ss"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PricePrediction\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__r5IYeQx36D"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1692950872680,
     "user": {
      "displayName": "anudeep anisetty",
      "userId": "13005275210132423070"
     },
     "user_tz": 420
    },
    "id": "ckM62AjqpVEE"
   },
   "outputs": [],
   "source": [
    "csv_file_path = \"Chicago_taxi_fare_5000.csv\"\n",
    "\n",
    "# Read data from the downloaded CSV file\n",
    "taxi_data = spark.read.csv(\"/content/\"+csv_file_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1692950874902,
     "user": {
      "displayName": "anudeep anisetty",
      "userId": "13005275210132423070"
     },
     "user_tz": 420
    },
    "id": "tTdYhk37lmW6"
   },
   "outputs": [],
   "source": [
    "# Clean the data and handle inconsistencies\n",
    "cleaned_data = taxi_data.filter(\n",
    "    col(\"trip_seconds\").isNotNull() &\n",
    "    col(\"trip_miles\").isNotNull() &\n",
    "    col(\"pickup_community_area\").isNotNull() &\n",
    "    col(\"fare\").isNotNull()\n",
    ")\n",
    "\n",
    "# Feature columns and assembler\n",
    "feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(cleaned_data)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = assembled_data.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"fare\")\n",
    "lr_model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 982,
     "status": "ok",
     "timestamp": 1692950877842,
     "user": {
      "displayName": "anudeep anisetty",
      "userId": "13005275210132423070"
     },
     "user_tz": 420
    },
    "id": "RvkjURv6luY2",
    "outputId": "90602299-9777-4a52-e642-042002cc05f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 13.325519339163872\n",
      "R2: 0.32574266647997696\n",
      "+----+-----------------+\n",
      "|fare|       prediction|\n",
      "+----+-----------------+\n",
      "|3.25|8.742912018173813|\n",
      "|3.25|  8.8407249708915|\n",
      "|3.25|8.873329288464063|\n",
      "|12.0|8.873329288464063|\n",
      "|3.25|8.905933606036626|\n",
      "|3.25|8.905933606036626|\n",
      "|3.25|8.905933606036626|\n",
      "|3.25|8.905933606036626|\n",
      "|3.25|8.905933606036626|\n",
      "|3.25|8.905933606036626|\n",
      "| 3.5|8.905933606036626|\n",
      "|3.25| 8.97114224118175|\n",
      "|80.0| 8.97114224118175|\n",
      "|3.25|9.036350876326877|\n",
      "|3.25|   9.101559511472|\n",
      "|3.25|9.134163829044564|\n",
      "|3.25|9.166768146617125|\n",
      "|3.25|9.166768146617125|\n",
      "|3.25|9.166768146617125|\n",
      "|3.25|9.297185416907375|\n",
      "+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_results = lr_model.evaluate(test_data)\n",
    "print(\"Root Mean Squared Error (RMSE):\", test_results.rootMeanSquaredError)\n",
    "print(\"R2:\", test_results.r2)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr_model.transform(test_data)\n",
    "predictions.select(\"fare\", \"prediction\").show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We executed our Spark application from the command line interface, initiating a job that performed data analysis and prediction using the Linear Regression model. As the job ran, we monitored its progress and outcome through Amazon Web Services (AWS) tools."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOQl+tA7W8MYjkaM4ZDar2f",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
